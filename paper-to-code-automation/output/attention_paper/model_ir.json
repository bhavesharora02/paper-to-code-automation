{
  "paper_metadata": {
    "title": "Attention Is All You Need AshishVaswani∗ NoamShazeer∗ NikiParmar∗ JakobUszkoreit∗ GoogleBrain GoogleBrain GoogleResearch GoogleResearch",
    "authors": [],
    "year": 2014,
    "url": null,
    "doi": null
  },
  "model": {
    "name": "default_model",
    "type": "nlp",
    "layers": [
      {
        "name": "embedding_1",
        "type": "embedding",
        "parameters": {},
        "description": "background: ThegoalofreducingsequentialcomputationalsoformsthefoundationoftheExtendedNeuralGPU [20],ByteNet[15]andConvS2S[8],allofwhichuseconvolutionalneuralnetworksasbasicbuilding block,computinghiddenrepresentationsinparallelforallinputandoutputpositions.Inthesemodels, thenumberofoperationsrequiredtorelatesignalsfromtwoarbitraryinputoroutputpositionsgrows inthedistancebetweenpositions,linearlyforConvS2SandlogarithmicallyforByteNet. Thismakes it more difficult to learn dependencies between distant positions [11]. In the Transformer this is reducedtoaconstantnumberofoperations, albeitatthecostofreducedeffectiveresolutiondue to averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as describedinsection3.2. Self-attention,sometimescalledintra-attentionisanattentionmechanismrelatingdifferentpositions ofasinglesequenceinordertocomputearepresentationofthesequence. Self-attentionhasbeen usedsuccessfullyinavarietyoftasksincludingreadingcomprehension,abstractivesummarization, textualentailmentandlearningtask-independentsentencerepresentations[4,22,23,19]. End-to-endmemorynetworksarebasedonarecurrentattentionmechanisminsteadofsequence- alignedrecurrenceandhavebeenshowntoperformwellonsimple-languagequestionansweringand languagemodelingtasks[28]. To the best of our knowledge, however, the Transformer is the first transduction model relying entirelyonself-attentiontocomputerepresentationsofitsinputandoutputwithoutusingsequence- alignedRNNsorconvolution. Inthefollowingsections,wewilldescribetheTransformer,motivate self-attentionanddiscussitsadvantagesovermodelssuchas[14,15]and[8]. 3 ModelArchitecture Mostcompetitiveneuralsequencetransductionmodelshaveanencoder-decoderstructure[5,2,29]. Here, the encoder maps an input sequence of symbol representations (x ,...,x ) to a sequence 1 n of continuous representations z = (z ,...,z ). Given z, the decoder then generates an output 1 n sequence(y ,...,y )ofsymbolsoneelementatatime. Ateachstepthemodelisauto-regressive 1 m [9],consumingthepreviouslygeneratedsymbolsasadditionalinputwhengeneratingthenext. TheTransformerfollowsthisoverallarchitectureusingstackedself-attentionandpoint-wise,fully connectedlayersforboththeencoderanddecoder,shownintheleftandrighthalvesofFigure1, respectively. 3.1 EncoderandDecoderStacks Encoder: The encoder is composed of a stack of N = 6 identical layers. Each layer has two sub-layers. Thefirstisamulti-headself-attentionmechanism,andthesecondisasimple,position- 2 Figure1: TheTransformer-modelarchitecture. wisefullyconnectedfeed-forwardnetwork. Weemployaresidualconnection[10]aroundeachof the two sub-layers, followed by layer normalization [1]. That is, the output of each sub-layer is LayerNorm(x+Sublayer(x)),whereSublayer(x)isthefunctionimplementedbythesub-layer itself. Tofacilitatetheseresidualconnections,allsub-layersinthemodel,aswellastheembedding layers,produceoutputsofdimensiond =512. model Decoder: ThedecoderisalsocomposedofastackofN =6identicallayers. Inadditiontothetwo sub-layersineachencoderlayer,thedecoderinsertsathirdsub-layer,whichperformsmulti-head attentionovertheoutputoftheencoderstack. Similartotheencoder,weemployresidualconnections aroundeachofthesub-layers,followedbylayernormalization. Wealsomodifytheself-attention sub-layer in the decoder stack to prevent positions from attending to subsequent positions. This masking,combinedwithfactthattheoutputembeddingsareoffsetbyoneposition,ensuresthatthe predictionsforpositionicandependonlyontheknownoutputsatpositionslessthani. 3.2 Attention Anattentionfunctioncanbedescribedasmappingaqueryandasetofkey-valuepairstoanoutput, wherethequery,keys,values,andoutputareallvectors. Theoutputiscomputedasaweightedsum ofthevalues,wheretheweightassignedtoeachvalueiscomputedbyacompatibilityfunctionofthe querywiththecorrespondingkey. 3.2.1 ScaledDot-ProductAttention Wecallourparticularattention\"ScaledDot-ProductAttention\"(Figure2). Theinputconsistsof queriesandkeysofdimensiond ,andvaluesofdimensiond . Wecomputethedotproductsofthe k v 3 ScaledDot-ProductAttention Multi-HeadAttention Figure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several attentionlayersrunninginparallel. √ querywithallkeys,divideeachby d ,andapplyasoftmaxfunctiontoobtaintheweightsonthe k values. Inpractice,wecomputetheattentionfunctiononasetofqueriessimultaneously,packedtogether intoamatrixQ. ThekeysandvaluesarealsopackedtogetherintomatricesK andV. Wecompute thematrixofoutputsas: QKT Attention(Q,K,V)=softmax( √ )V (1) d k Thetwomostcommonlyusedattentionfunctionsareadditiveattention[2],anddot-product(multi- plicative)attention. Dot-productattentionisidenticaltoouralgorithm,exceptforthescalingfactor of √1 . Additiveattentioncomputesthecompatibilityfunctionusingafeed-forwardnetworkwith dk asinglehiddenlayer. Whilethetwoaresimilarintheoreticalcomplexity,dot-productattentionis muchfasterandmorespace-efficientinpractice,sinceitcanbeimplementedusinghighlyoptimized matrixmultiplicationcode. Whileforsmallvaluesofd thetwomechanismsperformsimilarly,additiveattentionoutperforms k dotproductattentionwithoutscalingforlargervaluesofd [3]. Wesuspectthatforlargevaluesof k d ,thedotproductsgrowlargeinmagnitude,pushingthesoftmaxfunctionintoregionswhereithas k extremelysmallgradients4. Tocounteractthiseffect,wescalethedotproductsby √1 . dk 3.2.2 Multi-HeadAttention Insteadofperformingasingleattentionfunctionwithd -dimensionalkeys,valuesandqueries, model wefounditbeneficialtolinearlyprojectthequeries,keysandvalueshtimeswithdifferent,learned linearprojectionstod ,d andd dimensions,respectively. Oneachoftheseprojectedversionsof k k v queries,keysandvalueswethenperformtheattentionfunctioninparallel,yieldingd -dimensional v output values. These are concatenated and once again projected, resulting in the final values, as depictedinFigure2. Multi-headattentionallowsthemodeltojointlyattendtoinformationfromdifferentrepresentation subspacesatdifferentpositions. Withasingleattentionhead,averaginginhibitsthis. 4Toillustratewhythedotproductsgetlarge,assumethatthecomponentsofqandkareindependentrandom variableswithmean0andvariance1.Thentheirdotproduct,q·k= (cid:80)dk q k ,hasmean0andvarianced . i=1 i i k 4 MultiHead(Q,K,V)=Concat(head ,...,head )WO 1 h wherehead =Attention(QWQ,KWK,VWV) i i i i WheretheprojectionsareparametermatricesWQ ∈Rdmodel×dk,WK ∈Rdmodel×dk,WV ∈Rdmodel×dv i i i andWO ∈Rhdv×dmodel. In this work we employ h = 8 parallel attention layers, or heads. For each of these we use d =d =d /h=64. Duetothereduceddimensionofeachhead,thetotalcomputationalcost k v model issimilartothatofsingle-headattentionwithfulldimensionality. 3.2.3 ApplicationsofAttentioninourModel TheTransformerusesmulti-headattentioninthreedifferentways: • In\"encoder-decoderattention\"layers,thequeriescomefromthepreviousdecoderlayer, andthememorykeysandvaluescomefromtheoutputoftheencoder. Thisallowsevery positioninthedecodertoattendoverallpositionsintheinputsequence. Thismimicsthe typical encoder-decoder attention mechanisms in sequence-to-sequence models such as [31,2,8]. • Theencodercontainsself-attentionlayers. Inaself-attentionlayerallofthekeys,values andqueriescomefromthesameplace,inthiscase,theoutputofthepreviouslayerinthe encoder. Eachpositionintheencodercanattendtoallpositionsinthepreviouslayerofthe encoder. • Similarly,self-attentionlayersinthedecoderalloweachpositioninthedecodertoattendto allpositionsinthedecoderuptoandincludingthatposition. Weneedtopreventleftward informationflowinthedecodertopreservetheauto-regressiveproperty. Weimplementthis insideofscaleddot-productattentionbymaskingout(settingto−∞)allvaluesintheinput ofthesoftmaxwhichcorrespondtoillegalconnections. SeeFigure2. 3.3 Position-wiseFeed-ForwardNetworks Inadditiontoattentionsub-layers,eachofthelayersinourencoderanddecodercontainsafully connectedfeed-forwardnetwork,whichisappliedtoeachpositionseparatelyandidentically. This consistsoftwolineartransformationswithaReLUactivationinbetween. FFN(x)=max(0,xW +b )W +b (2) 1 1 2 2 Whilethelineartransformationsarethesameacrossdifferentpositions,theyusedifferentparameters from layer to layer. Another way of describing this is as two convolutions with kernel size 1. The dimensionality of input and output is d = 512, and the inner-layer has dimensionality model d =2048. ff 3.4 EmbeddingsandSoftmax Similarlytoothersequencetransductionmodels,weuselearnedembeddingstoconverttheinput tokensandoutputtokenstovectorsofdimensiond . Wealsousetheusuallearnedlineartransfor- model mationandsoftmaxfunctiontoconvertthedecoderoutputtopredictednext-tokenprobabilities. In ourmodel,wesharethesameweightmatrixbetweenthetwoembeddinglayersandthepre-√softmax lineartransformation,similarto[24]. Intheembeddinglayers,wemultiplythoseweightsby d . model 3.5 PositionalEncoding Sinceourmodelcontainsnorecurrenceandnoconvolution,inorderforthemodeltomakeuseofthe orderofthesequence,wemustinjectsomeinformationabouttherelativeorabsolutepositionofthe tokensinthesequence. Tothisend,weadd\"positionalencodings\"totheinputembeddingsatthe 5 Table1: Maximumpathlengths,per-layercomplexityandminimumnumberofsequentialoperations fordifferentlayertypes. nisthesequencelength,distherepresentationdimension,kisthekernel sizeofconvolutionsandrthesizeoftheneighborhoodinrestrictedself-attention. LayerType ComplexityperLayer Sequential MaximumPathLength Operations Self-Attention O(n2·d) O(1) O(1) Recurrent O(n·d2) O(n) O(n) Convolutional O(k·n·d2) O(1) O(log (n)) k Self-Attention(restricted) O(r·n·d) O(1) O(n/r) bottomsoftheencoderanddecoderstacks. Thepositionalencodingshavethesamedimensiond model astheembeddings,sothatthetwocanbesummed. Therearemanychoicesofpositionalencodings, learnedandfixed[8]. Inthiswork,weusesineandcosinefunctionsofdifferentfrequencies: PE =sin(pos/100002i/dmodel) (pos,2i) PE =cos(pos/100002i/dmodel) (pos,2i+1) whereposisthepositionandiisthedimension. Thatis,eachdimensionofthepositionalencoding correspondstoasinusoid. Thewavelengthsformageometricprogressionfrom2πto10000·2π. We chosethisfunctionbecausewehypothesizeditwouldallowthemodeltoeasilylearntoattendby relativepositions,sinceforanyfixedoffsetk,PE canberepresentedasalinearfunctionof pos+k PE . pos Wealsoexperimentedwithusinglearnedpositionalembeddings[8]instead,andfoundthatthetwo versionsproducednearlyidenticalresults(seeTable3row(E)).Wechosethesinusoidalversion becauseitmayallowthemodeltoextrapolatetosequencelengthslongerthantheonesencountered duringtraining. 4 WhySelf-Attention In this section we compare various aspects of self-attention layers to the recurrent and convolu- tionallayerscommonlyusedformappingonevariable-lengthsequenceofsymbolrepresentations (x ,...,x ) to another sequence of equal length (z ,...,z ), with x ,z ∈ Rd, such as a hidden 1 n 1 n i i layerinatypicalsequencetransductionencoderordecoder. Motivatingouruseofself-attentionwe considerthreedesiderata. Oneisthetotalcomputationalcomplexityperlayer. Anotheristheamountofcomputationthatcan beparallelized,asmeasuredbytheminimumnumberofsequentialoperationsrequired. Thethirdisthepathlengthbetweenlong-rangedependenciesinthenetwork. Learninglong-range dependenciesisakeychallengeinmanysequencetransductiontasks. Onekeyfactoraffectingthe abilitytolearnsuchdependenciesisthelengthofthepathsforwardandbackwardsignalshaveto traverseinthenetwork. Theshorterthesepathsbetweenanycombinationofpositionsintheinput andoutputsequences,theeasieritistolearnlong-rangedependencies[11]. Hencewealsocompare themaximumpathlengthbetweenanytwoinputandoutputpositionsinnetworkscomposedofthe differentlayertypes. AsnotedinTable1,aself-attentionlayerconnectsallpositionswithaconstantnumberofsequentially executed operations, whereas a recurrent layer requires O(n) sequential operations. In terms of computationalcomplexity,self-attentionlayersarefasterthanrecurrentlayerswhenthesequence length n is smaller than the representation dimensionality d, which is most often the case with sentencerepresentationsusedbystate-of-the-artmodelsinmachinetranslations,suchasword-piece [31]andbyte-pair[25]representations. Toimprovecomputationalperformancefortasksinvolving verylongsequences,self-attentioncouldberestrictedtoconsideringonlyaneighborhoodofsizerin 6 theinputsequencecenteredaroundtherespectiveoutputposition. Thiswouldincreasethemaximum pathlengthtoO(n/r). Weplantoinvestigatethisapproachfurtherinfuturework. Asingleconvolutionallayerwithkernelwidthk <ndoesnotconnectallpairsofinputandoutput positions. DoingsorequiresastackofO(n/k)convolutionallayersinthecaseofcontiguouskernels, orO(log (n))inthecaseofdilatedconvolutions[15], increasingthelengthofthelongestpaths k betweenanytwopositionsinthenetwork. Convolutionallayersaregenerallymoreexpensivethan recurrent layers, by a factor of k. Separable convolutions [6], however, decrease the complexity considerably, toO(k·n·d+n·d2). Evenwithk = n, however, thecomplexityofaseparable convolutionisequaltothecombinationofaself-attentionlayerandapoint-wisefeed-forwardlayer, theapproachwetakeinourmodel. Assidebenefit,self-attentioncouldyieldmoreinterpretablemodels.Weinspectattentiondistributions fromourmodelsandpresentanddiscussexamplesintheappendix. Notonlydoindividualattention headsclearlylearntoperformdifferenttasks,manyappeartoexhibitbehaviorrelatedtothesyntactic andsemanticstructureofthesentences. 5 Training Thissectiondescribesthetrainingregimeforourmodels. 5.1 TrainingDataandBatching We trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million sentencepairs. Sentenceswereencodedusingbyte-pairencoding[3],whichhasasharedsource- targetvocabularyofabout37000tokens. ForEnglish-French,weusedthesignificantlylargerWMT 2014English-Frenchdatasetconsistingof36Msentencesandsplittokensintoa32000word-piece vocabulary[31].Sentencepairswerebatchedtogetherbyapproximatesequencelength.Eachtraining batchcontainedasetofsentencepairscontainingapproximately25000sourcetokensand25000 targettokens. 5.2 HardwareandSchedule Wetrainedourmodelsononemachinewith8NVIDIAP100GPUs. Forourbasemodelsusing thehyperparametersdescribedthroughoutthepaper,eachtrainingsteptookabout0.4seconds. We trainedthebasemodelsforatotalof100,000stepsor12hours. Forourbigmodels,(describedonthe bottomlineoftable3),steptimewas1.0seconds. Thebigmodelsweretrainedfor300,000steps (3.5days). 5.3 Optimizer WeusedtheAdamoptimizer[17]withβ =0.9,β =0.98and(cid:15)=10−9. Wevariedthelearning 1 2 rateoverthecourseoftraining,accordingtotheformula: lrate=d−0.5 ·min(step_num−0.5,step_num·warmup_steps−1.5) (3) model Thiscorrespondstoincreasingthelearningratelinearlyforthefirstwarmup_stepstrainingsteps, anddecreasingitthereafterproportionallytotheinversesquarerootofthestepnumber. Weused warmup_steps=4000. 5.4 Regularization Weemploythreetypesofregularizationduringtraining: ResidualDropout Weapplydropout[27]totheoutputofeachsub-layer,beforeitisaddedtothe sub-layerinputandnormalized. Inaddition,weapplydropouttothesumsoftheembeddingsandthe positionalencodingsinboththeencoderanddecoderstacks. Forthebasemodel,weusearateof P =0.1. drop 7 Table2: TheTransformerachievesbetterBLEUscoresthanpreviousstate-of-the-artmodelsonthe English-to-GermanandEnglish-to-Frenchnewstest2014testsatafractionofthetrainingcost. BLEU TrainingCost(FLOPs) Model EN-DE EN-FR EN-DE EN-FR ByteNet[15] 23.75 Deep-Att+PosUnk[32] 39.2 1.0·1020 GNMT+RL[31] 24.6 39.92 2.3·1019 1.4·1020 ConvS2S[8] 25.16 40.46 9.6·1018 1.5·1020 MoE[26] 26.03 40.56 2.0·1019 1.2·1020 Deep-Att+PosUnkEnsemble[32] 40.4 8.0·1020 GNMT+RLEnsemble[31] 26.30 41.16 1.8·1020 1.1·1021 ConvS2SEnsemble[8] 26.36 41.29 7.7·1019 1.2·1021 Transformer(basemodel) 27.3 38.1 3.3·1018 Transformer(big) 28.4 41.0 2.3·1019 LabelSmoothing Duringtraining,weemployedlabelsmoothingofvalue(cid:15) = 0.1[30]. This ls hurtsperplexity,asthemodellearnstobemoreunsure,butimprovesaccuracyandBLEUscore."
      },
      {
        "name": "transformer_2",
        "type": "transformer",
        "parameters": {},
        "description": "background: ThegoalofreducingsequentialcomputationalsoformsthefoundationoftheExtendedNeuralGPU [20],ByteNet[15]andConvS2S[8],allofwhichuseconvolutionalneuralnetworksasbasicbuilding block,computinghiddenrepresentationsinparallelforallinputandoutputpositions.Inthesemodels, thenumberofoperationsrequiredtorelatesignalsfromtwoarbitraryinputoroutputpositionsgrows inthedistancebetweenpositions,linearlyforConvS2SandlogarithmicallyforByteNet. Thismakes it more difficult to learn dependencies between distant positions [11]. In the Transformer this is reducedtoaconstantnumberofoperations, albeitatthecostofreducedeffectiveresolutiondue to averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as describedinsection3.2. Self-attention,sometimescalledintra-attentionisanattentionmechanismrelatingdifferentpositions ofasinglesequenceinordertocomputearepresentationofthesequence. Self-attentionhasbeen usedsuccessfullyinavarietyoftasksincludingreadingcomprehension,abstractivesummarization, textualentailmentandlearningtask-independentsentencerepresentations[4,22,23,19]. End-to-endmemorynetworksarebasedonarecurrentattentionmechanisminsteadofsequence- alignedrecurrenceandhavebeenshowntoperformwellonsimple-languagequestionansweringand languagemodelingtasks[28]. To the best of our knowledge, however, the Transformer is the first transduction model relying entirelyonself-attentiontocomputerepresentationsofitsinputandoutputwithoutusingsequence- alignedRNNsorconvolution. Inthefollowingsections,wewilldescribetheTransformer,motivate self-attentionanddiscussitsadvantagesovermodelssuchas[14,15]and[8]. 3 ModelArchitecture Mostcompetitiveneuralsequencetransductionmodelshaveanencoder-decoderstructure[5,2,29]. Here, the encoder maps an input sequence of symbol representations (x ,...,x ) to a sequence 1 n of continuous representations z = (z ,...,z ). Given z, the decoder then generates an output 1 n sequence(y ,...,y )ofsymbolsoneelementatatime. Ateachstepthemodelisauto-regressive 1 m [9],consumingthepreviouslygeneratedsymbolsasadditionalinputwhengeneratingthenext. TheTransformerfollowsthisoverallarchitectureusingstackedself-attentionandpoint-wise,fully connectedlayersforboththeencoderanddecoder,shownintheleftandrighthalvesofFigure1, respectively. 3.1 EncoderandDecoderStacks Encoder: The encoder is composed of a stack of N = 6 identical layers. Each layer has two sub-layers. Thefirstisamulti-headself-attentionmechanism,andthesecondisasimple,position- 2 Figure1: TheTransformer-modelarchitecture. wisefullyconnectedfeed-forwardnetwork. Weemployaresidualconnection[10]aroundeachof the two sub-layers, followed by layer normalization [1]. That is, the output of each sub-layer is LayerNorm(x+Sublayer(x)),whereSublayer(x)isthefunctionimplementedbythesub-layer itself. Tofacilitatetheseresidualconnections,allsub-layersinthemodel,aswellastheembedding layers,produceoutputsofdimensiond =512. model Decoder: ThedecoderisalsocomposedofastackofN =6identicallayers. Inadditiontothetwo sub-layersineachencoderlayer,thedecoderinsertsathirdsub-layer,whichperformsmulti-head attentionovertheoutputoftheencoderstack. Similartotheencoder,weemployresidualconnections aroundeachofthesub-layers,followedbylayernormalization. Wealsomodifytheself-attention sub-layer in the decoder stack to prevent positions from attending to subsequent positions. This masking,combinedwithfactthattheoutputembeddingsareoffsetbyoneposition,ensuresthatthe predictionsforpositionicandependonlyontheknownoutputsatpositionslessthani. 3.2 Attention Anattentionfunctioncanbedescribedasmappingaqueryandasetofkey-valuepairstoanoutput, wherethequery,keys,values,andoutputareallvectors. Theoutputiscomputedasaweightedsum ofthevalues,wheretheweightassignedtoeachvalueiscomputedbyacompatibilityfunctionofthe querywiththecorrespondingkey. 3.2.1 ScaledDot-ProductAttention Wecallourparticularattention\"ScaledDot-ProductAttention\"(Figure2). Theinputconsistsof queriesandkeysofdimensiond ,andvaluesofdimensiond . Wecomputethedotproductsofthe k v 3 ScaledDot-ProductAttention Multi-HeadAttention Figure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several attentionlayersrunninginparallel. √ querywithallkeys,divideeachby d ,andapplyasoftmaxfunctiontoobtaintheweightsonthe k values. Inpractice,wecomputetheattentionfunctiononasetofqueriessimultaneously,packedtogether intoamatrixQ. ThekeysandvaluesarealsopackedtogetherintomatricesK andV. Wecompute thematrixofoutputsas: QKT Attention(Q,K,V)=softmax( √ )V (1) d k Thetwomostcommonlyusedattentionfunctionsareadditiveattention[2],anddot-product(multi- plicative)attention. Dot-productattentionisidenticaltoouralgorithm,exceptforthescalingfactor of √1 . Additiveattentioncomputesthecompatibilityfunctionusingafeed-forwardnetworkwith dk asinglehiddenlayer. Whilethetwoaresimilarintheoreticalcomplexity,dot-productattentionis muchfasterandmorespace-efficientinpractice,sinceitcanbeimplementedusinghighlyoptimized matrixmultiplicationcode. Whileforsmallvaluesofd thetwomechanismsperformsimilarly,additiveattentionoutperforms k dotproductattentionwithoutscalingforlargervaluesofd [3]. Wesuspectthatforlargevaluesof k d ,thedotproductsgrowlargeinmagnitude,pushingthesoftmaxfunctionintoregionswhereithas k extremelysmallgradients4. Tocounteractthiseffect,wescalethedotproductsby √1 . dk 3.2.2 Multi-HeadAttention Insteadofperformingasingleattentionfunctionwithd -dimensionalkeys,valuesandqueries, model wefounditbeneficialtolinearlyprojectthequeries,keysandvalueshtimeswithdifferent,learned linearprojectionstod ,d andd dimensions,respectively. Oneachoftheseprojectedversionsof k k v queries,keysandvalueswethenperformtheattentionfunctioninparallel,yieldingd -dimensional v output values. These are concatenated and once again projected, resulting in the final values, as depictedinFigure2. Multi-headattentionallowsthemodeltojointlyattendtoinformationfromdifferentrepresentation subspacesatdifferentpositions. Withasingleattentionhead,averaginginhibitsthis. 4Toillustratewhythedotproductsgetlarge,assumethatthecomponentsofqandkareindependentrandom variableswithmean0andvariance1.Thentheirdotproduct,q·k= (cid:80)dk q k ,hasmean0andvarianced . i=1 i i k 4 MultiHead(Q,K,V)=Concat(head ,...,head )WO 1 h wherehead =Attention(QWQ,KWK,VWV) i i i i WheretheprojectionsareparametermatricesWQ ∈Rdmodel×dk,WK ∈Rdmodel×dk,WV ∈Rdmodel×dv i i i andWO ∈Rhdv×dmodel. In this work we employ h = 8 parallel attention layers, or heads. For each of these we use d =d =d /h=64. Duetothereduceddimensionofeachhead,thetotalcomputationalcost k v model issimilartothatofsingle-headattentionwithfulldimensionality. 3.2.3 ApplicationsofAttentioninourModel TheTransformerusesmulti-headattentioninthreedifferentways: • In\"encoder-decoderattention\"layers,thequeriescomefromthepreviousdecoderlayer, andthememorykeysandvaluescomefromtheoutputoftheencoder. Thisallowsevery positioninthedecodertoattendoverallpositionsintheinputsequence. Thismimicsthe typical encoder-decoder attention mechanisms in sequence-to-sequence models such as [31,2,8]. • Theencodercontainsself-attentionlayers. Inaself-attentionlayerallofthekeys,values andqueriescomefromthesameplace,inthiscase,theoutputofthepreviouslayerinthe encoder. Eachpositionintheencodercanattendtoallpositionsinthepreviouslayerofthe encoder. • Similarly,self-attentionlayersinthedecoderalloweachpositioninthedecodertoattendto allpositionsinthedecoderuptoandincludingthatposition. Weneedtopreventleftward informationflowinthedecodertopreservetheauto-regressiveproperty. Weimplementthis insideofscaleddot-productattentionbymaskingout(settingto−∞)allvaluesintheinput ofthesoftmaxwhichcorrespondtoillegalconnections. SeeFigure2. 3.3 Position-wiseFeed-ForwardNetworks Inadditiontoattentionsub-layers,eachofthelayersinourencoderanddecodercontainsafully connectedfeed-forwardnetwork,whichisappliedtoeachpositionseparatelyandidentically. This consistsoftwolineartransformationswithaReLUactivationinbetween. FFN(x)=max(0,xW +b )W +b (2) 1 1 2 2 Whilethelineartransformationsarethesameacrossdifferentpositions,theyusedifferentparameters from layer to layer. Another way of describing this is as two convolutions with kernel size 1. The dimensionality of input and output is d = 512, and the inner-layer has dimensionality model d =2048. ff 3.4 EmbeddingsandSoftmax Similarlytoothersequencetransductionmodels,weuselearnedembeddingstoconverttheinput tokensandoutputtokenstovectorsofdimensiond . Wealsousetheusuallearnedlineartransfor- model mationandsoftmaxfunctiontoconvertthedecoderoutputtopredictednext-tokenprobabilities. In ourmodel,wesharethesameweightmatrixbetweenthetwoembeddinglayersandthepre-√softmax lineartransformation,similarto[24]. Intheembeddinglayers,wemultiplythoseweightsby d . model 3.5 PositionalEncoding Sinceourmodelcontainsnorecurrenceandnoconvolution,inorderforthemodeltomakeuseofthe orderofthesequence,wemustinjectsomeinformationabouttherelativeorabsolutepositionofthe tokensinthesequence. Tothisend,weadd\"positionalencodings\"totheinputembeddingsatthe 5 Table1: Maximumpathlengths,per-layercomplexityandminimumnumberofsequentialoperations fordifferentlayertypes. nisthesequencelength,distherepresentationdimension,kisthekernel sizeofconvolutionsandrthesizeoftheneighborhoodinrestrictedself-attention. LayerType ComplexityperLayer Sequential MaximumPathLength Operations Self-Attention O(n2·d) O(1) O(1) Recurrent O(n·d2) O(n) O(n) Convolutional O(k·n·d2) O(1) O(log (n)) k Self-Attention(restricted) O(r·n·d) O(1) O(n/r) bottomsoftheencoderanddecoderstacks. Thepositionalencodingshavethesamedimensiond model astheembeddings,sothatthetwocanbesummed. Therearemanychoicesofpositionalencodings, learnedandfixed[8]. Inthiswork,weusesineandcosinefunctionsofdifferentfrequencies: PE =sin(pos/100002i/dmodel) (pos,2i) PE =cos(pos/100002i/dmodel) (pos,2i+1) whereposisthepositionandiisthedimension. Thatis,eachdimensionofthepositionalencoding correspondstoasinusoid. Thewavelengthsformageometricprogressionfrom2πto10000·2π. We chosethisfunctionbecausewehypothesizeditwouldallowthemodeltoeasilylearntoattendby relativepositions,sinceforanyfixedoffsetk,PE canberepresentedasalinearfunctionof pos+k PE . pos Wealsoexperimentedwithusinglearnedpositionalembeddings[8]instead,andfoundthatthetwo versionsproducednearlyidenticalresults(seeTable3row(E)).Wechosethesinusoidalversion becauseitmayallowthemodeltoextrapolatetosequencelengthslongerthantheonesencountered duringtraining. 4 WhySelf-Attention In this section we compare various aspects of self-attention layers to the recurrent and convolu- tionallayerscommonlyusedformappingonevariable-lengthsequenceofsymbolrepresentations (x ,...,x ) to another sequence of equal length (z ,...,z ), with x ,z ∈ Rd, such as a hidden 1 n 1 n i i layerinatypicalsequencetransductionencoderordecoder. Motivatingouruseofself-attentionwe considerthreedesiderata. Oneisthetotalcomputationalcomplexityperlayer. Anotheristheamountofcomputationthatcan beparallelized,asmeasuredbytheminimumnumberofsequentialoperationsrequired. Thethirdisthepathlengthbetweenlong-rangedependenciesinthenetwork. Learninglong-range dependenciesisakeychallengeinmanysequencetransductiontasks. Onekeyfactoraffectingthe abilitytolearnsuchdependenciesisthelengthofthepathsforwardandbackwardsignalshaveto traverseinthenetwork. Theshorterthesepathsbetweenanycombinationofpositionsintheinput andoutputsequences,theeasieritistolearnlong-rangedependencies[11]. Hencewealsocompare themaximumpathlengthbetweenanytwoinputandoutputpositionsinnetworkscomposedofthe differentlayertypes. AsnotedinTable1,aself-attentionlayerconnectsallpositionswithaconstantnumberofsequentially executed operations, whereas a recurrent layer requires O(n) sequential operations. In terms of computationalcomplexity,self-attentionlayersarefasterthanrecurrentlayerswhenthesequence length n is smaller than the representation dimensionality d, which is most often the case with sentencerepresentationsusedbystate-of-the-artmodelsinmachinetranslations,suchasword-piece [31]andbyte-pair[25]representations. Toimprovecomputationalperformancefortasksinvolving verylongsequences,self-attentioncouldberestrictedtoconsideringonlyaneighborhoodofsizerin 6 theinputsequencecenteredaroundtherespectiveoutputposition. Thiswouldincreasethemaximum pathlengthtoO(n/r). Weplantoinvestigatethisapproachfurtherinfuturework. Asingleconvolutionallayerwithkernelwidthk <ndoesnotconnectallpairsofinputandoutput positions. DoingsorequiresastackofO(n/k)convolutionallayersinthecaseofcontiguouskernels, orO(log (n))inthecaseofdilatedconvolutions[15], increasingthelengthofthelongestpaths k betweenanytwopositionsinthenetwork. Convolutionallayersaregenerallymoreexpensivethan recurrent layers, by a factor of k. Separable convolutions [6], however, decrease the complexity considerably, toO(k·n·d+n·d2). Evenwithk = n, however, thecomplexityofaseparable convolutionisequaltothecombinationofaself-attentionlayerandapoint-wisefeed-forwardlayer, theapproachwetakeinourmodel. Assidebenefit,self-attentioncouldyieldmoreinterpretablemodels.Weinspectattentiondistributions fromourmodelsandpresentanddiscussexamplesintheappendix. Notonlydoindividualattention headsclearlylearntoperformdifferenttasks,manyappeartoexhibitbehaviorrelatedtothesyntactic andsemanticstructureofthesentences. 5 Training Thissectiondescribesthetrainingregimeforourmodels. 5.1 TrainingDataandBatching We trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million sentencepairs. Sentenceswereencodedusingbyte-pairencoding[3],whichhasasharedsource- targetvocabularyofabout37000tokens. ForEnglish-French,weusedthesignificantlylargerWMT 2014English-Frenchdatasetconsistingof36Msentencesandsplittokensintoa32000word-piece vocabulary[31].Sentencepairswerebatchedtogetherbyapproximatesequencelength.Eachtraining batchcontainedasetofsentencepairscontainingapproximately25000sourcetokensand25000 targettokens. 5.2 HardwareandSchedule Wetrainedourmodelsononemachinewith8NVIDIAP100GPUs. Forourbasemodelsusing thehyperparametersdescribedthroughoutthepaper,eachtrainingsteptookabout0.4seconds. We trainedthebasemodelsforatotalof100,000stepsor12hours. Forourbigmodels,(describedonthe bottomlineoftable3),steptimewas1.0seconds. Thebigmodelsweretrainedfor300,000steps (3.5days). 5.3 Optimizer WeusedtheAdamoptimizer[17]withβ =0.9,β =0.98and(cid:15)=10−9. Wevariedthelearning 1 2 rateoverthecourseoftraining,accordingtotheformula: lrate=d−0.5 ·min(step_num−0.5,step_num·warmup_steps−1.5) (3) model Thiscorrespondstoincreasingthelearningratelinearlyforthefirstwarmup_stepstrainingsteps, anddecreasingitthereafterproportionallytotheinversesquarerootofthestepnumber. Weused warmup_steps=4000. 5.4 Regularization Weemploythreetypesofregularizationduringtraining: ResidualDropout Weapplydropout[27]totheoutputofeachsub-layer,beforeitisaddedtothe sub-layerinputandnormalized. Inaddition,weapplydropouttothesumsoftheembeddingsandthe positionalencodingsinboththeencoderanddecoderstacks. Forthebasemodel,weusearateof P =0.1. drop 7 Table2: TheTransformerachievesbetterBLEUscoresthanpreviousstate-of-the-artmodelsonthe English-to-GermanandEnglish-to-Frenchnewstest2014testsatafractionofthetrainingcost. BLEU TrainingCost(FLOPs) Model EN-DE EN-FR EN-DE EN-FR ByteNet[15] 23.75 Deep-Att+PosUnk[32] 39.2 1.0·1020 GNMT+RL[31] 24.6 39.92 2.3·1019 1.4·1020 ConvS2S[8] 25.16 40.46 9.6·1018 1.5·1020 MoE[26] 26.03 40.56 2.0·1019 1.2·1020 Deep-Att+PosUnkEnsemble[32] 40.4 8.0·1020 GNMT+RLEnsemble[31] 26.30 41.16 1.8·1020 1.1·1021 ConvS2SEnsemble[8] 26.36 41.29 7.7·1019 1.2·1021 Transformer(basemodel) 27.3 38.1 3.3·1018 Transformer(big) 28.4 41.0 2.3·1019 LabelSmoothing Duringtraining,weemployedlabelsmoothingofvalue(cid:15) = 0.1[30]. This ls hurtsperplexity,asthemodellearnstobemoreunsure,butimprovesaccuracyandBLEUscore."
      }
    ],
    "input_spec": {},
    "output_spec": {},
    "description": null
  },
  "training": {
    "dataset": {
      "name": "default_dataset",
      "input_shape": [
        "batch_size",
        "seq_len"
      ],
      "output_shape": [
        "batch_size",
        "num_classes"
      ],
      "preprocessing": {},
      "source": null
    },
    "batch_size": 32,
    "epochs": 10,
    "optimizer": {
      "type": "adam",
      "parameters": {
        "lr": 0.001
      }
    },
    "loss": "cross_entropy",
    "metrics": [],
    "learning_rate": 0.0001
  },
  "evaluation": {
    "accuracy": 90.5
  },
  "created_at": "2025-08-16T12:37:35.911832",
  "version": "1.0.0"
}