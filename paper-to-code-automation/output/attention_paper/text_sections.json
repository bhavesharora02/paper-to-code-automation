{
  "title": "Attention Is All You Need AshishVaswani\u2217 NoamShazeer\u2217 NikiParmar\u2217 JakobUszkoreit\u2217 GoogleBrain GoogleBrain GoogleResearch GoogleResearch",
  "abstract": "Thedominantsequencetransductionmodelsarebasedoncomplexrecurrentor convolutionalneuralnetworksthatincludeanencoderandadecoder. Thebest performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, basedsolelyonattentionmechanisms,dispensingwithrecurrenceandconvolutions entirely. Experiments on two machine translation tasks show these models to besuperiorinqualitywhilebeingmoreparallelizableandrequiringsignificantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English- to-German translation task, improving over the existing best results, including ensembles,byover2BLEU.OntheWMT2014English-to-Frenchtranslationtask, ourmodelestablishesanewsingle-modelstate-of-the-artBLEUscoreof41.0after trainingfor3.5daysoneightGPUs,asmallfractionofthetrainingcostsofthe bestmodelsfromtheliterature.",
  "sections": {
    "preamble": "Attention Is All You Need AshishVaswani\u2217 NoamShazeer\u2217 NikiParmar\u2217 JakobUszkoreit\u2217 GoogleBrain GoogleBrain GoogleResearch GoogleResearch avaswani@google.com noam@google.com nikip@google.com usz@google.com LlionJones\u2217 AidanN.Gomez\u2217 \u2020 \u0141ukaszKaiser\u2217 GoogleResearch UniversityofToronto GoogleBrain llion@google.com aidan@cs.toronto.edu lukaszkaiser@google.com IlliaPolosukhin\u2217 \u2021 illia.polosukhin@gmail.com",
    "abstract": "Thedominantsequencetransductionmodelsarebasedoncomplexrecurrentor convolutionalneuralnetworksthatincludeanencoderandadecoder. Thebest performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, basedsolelyonattentionmechanisms,dispensingwithrecurrenceandconvolutions entirely. Experiments on two machine translation tasks show these models to besuperiorinqualitywhilebeingmoreparallelizableandrequiringsignificantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English- to-German translation task, improving over the existing best results, including ensembles,byover2BLEU.OntheWMT2014English-to-Frenchtranslationtask, ourmodelestablishesanewsingle-modelstate-of-the-artBLEUscoreof41.0after trainingfor3.5daysoneightGPUs,asmallfractionofthetrainingcostsofthe bestmodelsfromtheliterature.",
    "introduction": "Recurrentneuralnetworks,longshort-termmemory[12]andgatedrecurrent[7]neuralnetworks inparticular,havebeenfirmlyestablishedasstateoftheartapproachesinsequencemodelingand transductionproblemssuchaslanguagemodelingandmachinetranslation[29,2,5]. Numerous effortshavesincecontinuedtopushtheboundariesofrecurrentlanguagemodelsandencoder-decoder architectures[31,21,13]. \u2217Equalcontribution.Listingorderisrandom.JakobproposedreplacingRNNswithself-attentionandstarted theefforttoevaluatethisidea.Ashish,withIllia,designedandimplementedthefirstTransformermodelsand hasbeencruciallyinvolvedineveryaspectofthiswork.Noamproposedscaleddot-productattention,multi-head attentionandtheparameter-freepositionrepresentationandbecametheotherpersoninvolvedinnearlyevery detail.Nikidesigned,implemented,tunedandevaluatedcountlessmodelvariantsinouroriginalcodebaseand tensor2tensor.Llionalsoexperimentedwithnovelmodelvariants,wasresponsibleforourinitialcodebase,and efficientinferenceandvisualizations.LukaszandAidanspentcountlesslongdaysdesigningvariouspartsofand implementingtensor2tensor,replacingourearliercodebase,greatlyimprovingresultsandmassivelyaccelerating ourresearch. \u2020WorkperformedwhileatGoogleBrain. \u2021WorkperformedwhileatGoogleResearch. 31stConferenceonNeuralInformationProcessingSystems(NIPS2017),LongBeach,CA,USA. Recurrentmodelstypicallyfactorcomputationalongthesymbolpositionsoftheinputandoutput sequences. Aligningthepositionstostepsincomputationtime,theygenerateasequenceofhidden statesh ,asafunctionoftheprevioushiddenstateh andtheinputforpositiont. Thisinherently t t\u22121 sequentialnatureprecludesparallelizationwithintrainingexamples,whichbecomescriticalatlonger sequencelengths,asmemoryconstraintslimitbatchingacrossexamples. Recentworkhasachieved significantimprovementsincomputationalefficiencythroughfactorizationtricks[18]andconditional computation[26],whilealsoimprovingmodelperformanceincaseofthelatter. Thefundamental constraintofsequentialcomputation,however,remains. Attentionmechanismshavebecomeanintegralpartofcompellingsequencemodelingandtransduc- tionmodelsinvarioustasks,allowingmodelingofdependencieswithoutregardtotheirdistancein theinputoroutputsequences[2,16]. Inallbutafewcases[22],however,suchattentionmechanisms areusedinconjunctionwitharecurrentnetwork. InthisworkweproposetheTransformer,amodelarchitectureeschewingrecurrenceandinstead relyingentirelyonanattentionmechanismtodrawglobaldependenciesbetweeninputandoutput. TheTransformerallowsforsignificantlymoreparallelizationandcanreachanewstateoftheartin translationqualityafterbeingtrainedforaslittleastwelvehoursoneightP100GPUs.",
    "background": "ThegoalofreducingsequentialcomputationalsoformsthefoundationoftheExtendedNeuralGPU [20],ByteNet[15]andConvS2S[8],allofwhichuseconvolutionalneuralnetworksasbasicbuilding block,computinghiddenrepresentationsinparallelforallinputandoutputpositions.Inthesemodels, thenumberofoperationsrequiredtorelatesignalsfromtwoarbitraryinputoroutputpositionsgrows inthedistancebetweenpositions,linearlyforConvS2SandlogarithmicallyforByteNet. Thismakes it more difficult to learn dependencies between distant positions [11]. In the Transformer this is reducedtoaconstantnumberofoperations, albeitatthecostofreducedeffectiveresolutiondue to averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as describedinsection3.2. Self-attention,sometimescalledintra-attentionisanattentionmechanismrelatingdifferentpositions ofasinglesequenceinordertocomputearepresentationofthesequence. Self-attentionhasbeen usedsuccessfullyinavarietyoftasksincludingreadingcomprehension,abstractivesummarization, textualentailmentandlearningtask-independentsentencerepresentations[4,22,23,19]. End-to-endmemorynetworksarebasedonarecurrentattentionmechanisminsteadofsequence- alignedrecurrenceandhavebeenshowntoperformwellonsimple-languagequestionansweringand languagemodelingtasks[28]. To the best of our knowledge, however, the Transformer is the first transduction model relying entirelyonself-attentiontocomputerepresentationsofitsinputandoutputwithoutusingsequence- alignedRNNsorconvolution. Inthefollowingsections,wewilldescribetheTransformer,motivate self-attentionanddiscussitsadvantagesovermodelssuchas[14,15]and[8]. 3 ModelArchitecture Mostcompetitiveneuralsequencetransductionmodelshaveanencoder-decoderstructure[5,2,29]. Here, the encoder maps an input sequence of symbol representations (x ,...,x ) to a sequence 1 n of continuous representations z = (z ,...,z ). Given z, the decoder then generates an output 1 n sequence(y ,...,y )ofsymbolsoneelementatatime. Ateachstepthemodelisauto-regressive 1 m [9],consumingthepreviouslygeneratedsymbolsasadditionalinputwhengeneratingthenext. TheTransformerfollowsthisoverallarchitectureusingstackedself-attentionandpoint-wise,fully connectedlayersforboththeencoderanddecoder,shownintheleftandrighthalvesofFigure1, respectively. 3.1 EncoderandDecoderStacks Encoder: The encoder is composed of a stack of N = 6 identical layers. Each layer has two sub-layers. Thefirstisamulti-headself-attentionmechanism,andthesecondisasimple,position- 2 Figure1: TheTransformer-modelarchitecture. wisefullyconnectedfeed-forwardnetwork. Weemployaresidualconnection[10]aroundeachof the two sub-layers, followed by layer normalization [1]. That is, the output of each sub-layer is LayerNorm(x+Sublayer(x)),whereSublayer(x)isthefunctionimplementedbythesub-layer itself. Tofacilitatetheseresidualconnections,allsub-layersinthemodel,aswellastheembedding layers,produceoutputsofdimensiond =512. model Decoder: ThedecoderisalsocomposedofastackofN =6identicallayers. Inadditiontothetwo sub-layersineachencoderlayer,thedecoderinsertsathirdsub-layer,whichperformsmulti-head attentionovertheoutputoftheencoderstack. Similartotheencoder,weemployresidualconnections aroundeachofthesub-layers,followedbylayernormalization. Wealsomodifytheself-attention sub-layer in the decoder stack to prevent positions from attending to subsequent positions. This masking,combinedwithfactthattheoutputembeddingsareoffsetbyoneposition,ensuresthatthe predictionsforpositionicandependonlyontheknownoutputsatpositionslessthani. 3.2 Attention Anattentionfunctioncanbedescribedasmappingaqueryandasetofkey-valuepairstoanoutput, wherethequery,keys,values,andoutputareallvectors. Theoutputiscomputedasaweightedsum ofthevalues,wheretheweightassignedtoeachvalueiscomputedbyacompatibilityfunctionofthe querywiththecorrespondingkey. 3.2.1 ScaledDot-ProductAttention Wecallourparticularattention\"ScaledDot-ProductAttention\"(Figure2). Theinputconsistsof queriesandkeysofdimensiond ,andvaluesofdimensiond . Wecomputethedotproductsofthe k v 3 ScaledDot-ProductAttention Multi-HeadAttention Figure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several attentionlayersrunninginparallel. \u221a querywithallkeys,divideeachby d ,andapplyasoftmaxfunctiontoobtaintheweightsonthe k values. Inpractice,wecomputetheattentionfunctiononasetofqueriessimultaneously,packedtogether intoamatrixQ. ThekeysandvaluesarealsopackedtogetherintomatricesK andV. Wecompute thematrixofoutputsas: QKT Attention(Q,K,V)=softmax( \u221a )V (1) d k Thetwomostcommonlyusedattentionfunctionsareadditiveattention[2],anddot-product(multi- plicative)attention. Dot-productattentionisidenticaltoouralgorithm,exceptforthescalingfactor of \u221a1 . Additiveattentioncomputesthecompatibilityfunctionusingafeed-forwardnetworkwith dk asinglehiddenlayer. Whilethetwoaresimilarintheoreticalcomplexity,dot-productattentionis muchfasterandmorespace-efficientinpractice,sinceitcanbeimplementedusinghighlyoptimized matrixmultiplicationcode. Whileforsmallvaluesofd thetwomechanismsperformsimilarly,additiveattentionoutperforms k dotproductattentionwithoutscalingforlargervaluesofd [3]. Wesuspectthatforlargevaluesof k d ,thedotproductsgrowlargeinmagnitude,pushingthesoftmaxfunctionintoregionswhereithas k extremelysmallgradients4. Tocounteractthiseffect,wescalethedotproductsby \u221a1 . dk 3.2.2 Multi-HeadAttention Insteadofperformingasingleattentionfunctionwithd -dimensionalkeys,valuesandqueries, model wefounditbeneficialtolinearlyprojectthequeries,keysandvalueshtimeswithdifferent,learned linearprojectionstod ,d andd dimensions,respectively. Oneachoftheseprojectedversionsof k k v queries,keysandvalueswethenperformtheattentionfunctioninparallel,yieldingd -dimensional v output values. These are concatenated and once again projected, resulting in the final values, as depictedinFigure2. Multi-headattentionallowsthemodeltojointlyattendtoinformationfromdifferentrepresentation subspacesatdifferentpositions. Withasingleattentionhead,averaginginhibitsthis. 4Toillustratewhythedotproductsgetlarge,assumethatthecomponentsofqandkareindependentrandom variableswithmean0andvariance1.Thentheirdotproduct,q\u00b7k= (cid:80)dk q k ,hasmean0andvarianced . i=1 i i k 4 MultiHead(Q,K,V)=Concat(head ,...,head )WO 1 h wherehead =Attention(QWQ,KWK,VWV) i i i i WheretheprojectionsareparametermatricesWQ \u2208Rdmodel\u00d7dk,WK \u2208Rdmodel\u00d7dk,WV \u2208Rdmodel\u00d7dv i i i andWO \u2208Rhdv\u00d7dmodel. In this work we employ h = 8 parallel attention layers, or heads. For each of these we use d =d =d /h=64. Duetothereduceddimensionofeachhead,thetotalcomputationalcost k v model issimilartothatofsingle-headattentionwithfulldimensionality. 3.2.3 ApplicationsofAttentioninourModel TheTransformerusesmulti-headattentioninthreedifferentways: \u2022 In\"encoder-decoderattention\"layers,thequeriescomefromthepreviousdecoderlayer, andthememorykeysandvaluescomefromtheoutputoftheencoder. Thisallowsevery positioninthedecodertoattendoverallpositionsintheinputsequence. Thismimicsthe typical encoder-decoder attention mechanisms in sequence-to-sequence models such as [31,2,8]. \u2022 Theencodercontainsself-attentionlayers. Inaself-attentionlayerallofthekeys,values andqueriescomefromthesameplace,inthiscase,theoutputofthepreviouslayerinthe encoder. Eachpositionintheencodercanattendtoallpositionsinthepreviouslayerofthe encoder. \u2022 Similarly,self-attentionlayersinthedecoderalloweachpositioninthedecodertoattendto allpositionsinthedecoderuptoandincludingthatposition. Weneedtopreventleftward informationflowinthedecodertopreservetheauto-regressiveproperty. Weimplementthis insideofscaleddot-productattentionbymaskingout(settingto\u2212\u221e)allvaluesintheinput ofthesoftmaxwhichcorrespondtoillegalconnections. SeeFigure2. 3.3 Position-wiseFeed-ForwardNetworks Inadditiontoattentionsub-layers,eachofthelayersinourencoderanddecodercontainsafully connectedfeed-forwardnetwork,whichisappliedtoeachpositionseparatelyandidentically. This consistsoftwolineartransformationswithaReLUactivationinbetween. FFN(x)=max(0,xW +b )W +b (2) 1 1 2 2 Whilethelineartransformationsarethesameacrossdifferentpositions,theyusedifferentparameters from layer to layer. Another way of describing this is as two convolutions with kernel size 1. The dimensionality of input and output is d = 512, and the inner-layer has dimensionality model d =2048. ff 3.4 EmbeddingsandSoftmax Similarlytoothersequencetransductionmodels,weuselearnedembeddingstoconverttheinput tokensandoutputtokenstovectorsofdimensiond . Wealsousetheusuallearnedlineartransfor- model mationandsoftmaxfunctiontoconvertthedecoderoutputtopredictednext-tokenprobabilities. In ourmodel,wesharethesameweightmatrixbetweenthetwoembeddinglayersandthepre-\u221asoftmax lineartransformation,similarto[24]. Intheembeddinglayers,wemultiplythoseweightsby d . model 3.5 PositionalEncoding Sinceourmodelcontainsnorecurrenceandnoconvolution,inorderforthemodeltomakeuseofthe orderofthesequence,wemustinjectsomeinformationabouttherelativeorabsolutepositionofthe tokensinthesequence. Tothisend,weadd\"positionalencodings\"totheinputembeddingsatthe 5 Table1: Maximumpathlengths,per-layercomplexityandminimumnumberofsequentialoperations fordifferentlayertypes. nisthesequencelength,distherepresentationdimension,kisthekernel sizeofconvolutionsandrthesizeoftheneighborhoodinrestrictedself-attention. LayerType ComplexityperLayer Sequential MaximumPathLength Operations Self-Attention O(n2\u00b7d) O(1) O(1) Recurrent O(n\u00b7d2) O(n) O(n) Convolutional O(k\u00b7n\u00b7d2) O(1) O(log (n)) k Self-Attention(restricted) O(r\u00b7n\u00b7d) O(1) O(n/r) bottomsoftheencoderanddecoderstacks. Thepositionalencodingshavethesamedimensiond model astheembeddings,sothatthetwocanbesummed. Therearemanychoicesofpositionalencodings, learnedandfixed[8]. Inthiswork,weusesineandcosinefunctionsofdifferentfrequencies: PE =sin(pos/100002i/dmodel) (pos,2i) PE =cos(pos/100002i/dmodel) (pos,2i+1) whereposisthepositionandiisthedimension. Thatis,eachdimensionofthepositionalencoding correspondstoasinusoid. Thewavelengthsformageometricprogressionfrom2\u03c0to10000\u00b72\u03c0. We chosethisfunctionbecausewehypothesizeditwouldallowthemodeltoeasilylearntoattendby relativepositions,sinceforanyfixedoffsetk,PE canberepresentedasalinearfunctionof pos+k PE . pos Wealsoexperimentedwithusinglearnedpositionalembeddings[8]instead,andfoundthatthetwo versionsproducednearlyidenticalresults(seeTable3row(E)).Wechosethesinusoidalversion becauseitmayallowthemodeltoextrapolatetosequencelengthslongerthantheonesencountered duringtraining. 4 WhySelf-Attention In this section we compare various aspects of self-attention layers to the recurrent and convolu- tionallayerscommonlyusedformappingonevariable-lengthsequenceofsymbolrepresentations (x ,...,x ) to another sequence of equal length (z ,...,z ), with x ,z \u2208 Rd, such as a hidden 1 n 1 n i i layerinatypicalsequencetransductionencoderordecoder. Motivatingouruseofself-attentionwe considerthreedesiderata. Oneisthetotalcomputationalcomplexityperlayer. Anotheristheamountofcomputationthatcan beparallelized,asmeasuredbytheminimumnumberofsequentialoperationsrequired. Thethirdisthepathlengthbetweenlong-rangedependenciesinthenetwork. Learninglong-range dependenciesisakeychallengeinmanysequencetransductiontasks. Onekeyfactoraffectingthe abilitytolearnsuchdependenciesisthelengthofthepathsforwardandbackwardsignalshaveto traverseinthenetwork. Theshorterthesepathsbetweenanycombinationofpositionsintheinput andoutputsequences,theeasieritistolearnlong-rangedependencies[11]. Hencewealsocompare themaximumpathlengthbetweenanytwoinputandoutputpositionsinnetworkscomposedofthe differentlayertypes. AsnotedinTable1,aself-attentionlayerconnectsallpositionswithaconstantnumberofsequentially executed operations, whereas a recurrent layer requires O(n) sequential operations. In terms of computationalcomplexity,self-attentionlayersarefasterthanrecurrentlayerswhenthesequence length n is smaller than the representation dimensionality d, which is most often the case with sentencerepresentationsusedbystate-of-the-artmodelsinmachinetranslations,suchasword-piece [31]andbyte-pair[25]representations. Toimprovecomputationalperformancefortasksinvolving verylongsequences,self-attentioncouldberestrictedtoconsideringonlyaneighborhoodofsizerin 6 theinputsequencecenteredaroundtherespectiveoutputposition. Thiswouldincreasethemaximum pathlengthtoO(n/r). Weplantoinvestigatethisapproachfurtherinfuturework. Asingleconvolutionallayerwithkernelwidthk <ndoesnotconnectallpairsofinputandoutput positions. DoingsorequiresastackofO(n/k)convolutionallayersinthecaseofcontiguouskernels, orO(log (n))inthecaseofdilatedconvolutions[15], increasingthelengthofthelongestpaths k betweenanytwopositionsinthenetwork. Convolutionallayersaregenerallymoreexpensivethan recurrent layers, by a factor of k. Separable convolutions [6], however, decrease the complexity considerably, toO(k\u00b7n\u00b7d+n\u00b7d2). Evenwithk = n, however, thecomplexityofaseparable convolutionisequaltothecombinationofaself-attentionlayerandapoint-wisefeed-forwardlayer, theapproachwetakeinourmodel. Assidebenefit,self-attentioncouldyieldmoreinterpretablemodels.Weinspectattentiondistributions fromourmodelsandpresentanddiscussexamplesintheappendix. Notonlydoindividualattention headsclearlylearntoperformdifferenttasks,manyappeartoexhibitbehaviorrelatedtothesyntactic andsemanticstructureofthesentences. 5 Training Thissectiondescribesthetrainingregimeforourmodels. 5.1 TrainingDataandBatching We trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million sentencepairs. Sentenceswereencodedusingbyte-pairencoding[3],whichhasasharedsource- targetvocabularyofabout37000tokens. ForEnglish-French,weusedthesignificantlylargerWMT 2014English-Frenchdatasetconsistingof36Msentencesandsplittokensintoa32000word-piece vocabulary[31].Sentencepairswerebatchedtogetherbyapproximatesequencelength.Eachtraining batchcontainedasetofsentencepairscontainingapproximately25000sourcetokensand25000 targettokens. 5.2 HardwareandSchedule Wetrainedourmodelsononemachinewith8NVIDIAP100GPUs. Forourbasemodelsusing thehyperparametersdescribedthroughoutthepaper,eachtrainingsteptookabout0.4seconds. We trainedthebasemodelsforatotalof100,000stepsor12hours. Forourbigmodels,(describedonthe bottomlineoftable3),steptimewas1.0seconds. Thebigmodelsweretrainedfor300,000steps (3.5days). 5.3 Optimizer WeusedtheAdamoptimizer[17]with\u03b2 =0.9,\u03b2 =0.98and(cid:15)=10\u22129. Wevariedthelearning 1 2 rateoverthecourseoftraining,accordingtotheformula: lrate=d\u22120.5 \u00b7min(step_num\u22120.5,step_num\u00b7warmup_steps\u22121.5) (3) model Thiscorrespondstoincreasingthelearningratelinearlyforthefirstwarmup_stepstrainingsteps, anddecreasingitthereafterproportionallytotheinversesquarerootofthestepnumber. Weused warmup_steps=4000. 5.4 Regularization Weemploythreetypesofregularizationduringtraining: ResidualDropout Weapplydropout[27]totheoutputofeachsub-layer,beforeitisaddedtothe sub-layerinputandnormalized. Inaddition,weapplydropouttothesumsoftheembeddingsandthe positionalencodingsinboththeencoderanddecoderstacks. Forthebasemodel,weusearateof P =0.1. drop 7 Table2: TheTransformerachievesbetterBLEUscoresthanpreviousstate-of-the-artmodelsonthe English-to-GermanandEnglish-to-Frenchnewstest2014testsatafractionofthetrainingcost. BLEU TrainingCost(FLOPs) Model EN-DE EN-FR EN-DE EN-FR ByteNet[15] 23.75 Deep-Att+PosUnk[32] 39.2 1.0\u00b71020 GNMT+RL[31] 24.6 39.92 2.3\u00b71019 1.4\u00b71020 ConvS2S[8] 25.16 40.46 9.6\u00b71018 1.5\u00b71020 MoE[26] 26.03 40.56 2.0\u00b71019 1.2\u00b71020 Deep-Att+PosUnkEnsemble[32] 40.4 8.0\u00b71020 GNMT+RLEnsemble[31] 26.30 41.16 1.8\u00b71020 1.1\u00b71021 ConvS2SEnsemble[8] 26.36 41.29 7.7\u00b71019 1.2\u00b71021 Transformer(basemodel) 27.3 38.1 3.3\u00b71018 Transformer(big) 28.4 41.0 2.3\u00b71019 LabelSmoothing Duringtraining,weemployedlabelsmoothingofvalue(cid:15) = 0.1[30]. This ls hurtsperplexity,asthemodellearnstobemoreunsure,butimprovesaccuracyandBLEUscore.",
    "results": "6.1 MachineTranslation OntheWMT2014English-to-Germantranslationtask,thebigtransformermodel(Transformer(big) inTable2)outperformsthebestpreviouslyreportedmodels(includingensembles)bymorethan2.0 BLEU,establishinganewstate-of-the-artBLEUscoreof28.4. Theconfigurationofthismodelis listedinthebottomlineofTable3. Trainingtook3.5dayson8P100GPUs. Evenourbasemodel surpassesallpreviouslypublishedmodelsandensembles,atafractionofthetrainingcostofanyof thecompetitivemodels. OntheWMT2014English-to-Frenchtranslationtask,ourbigmodelachievesaBLEUscoreof41.0, outperformingallofthepreviouslypublishedsinglemodels,atlessthan1/4thetrainingcostofthe previousstate-of-the-artmodel. TheTransformer(big)modeltrainedforEnglish-to-Frenchused dropoutrateP =0.1,insteadof0.3. drop Forthebasemodels,weusedasinglemodelobtainedbyaveragingthelast5checkpoints,which werewrittenat10-minuteintervals. Forthebigmodels,weaveragedthelast20checkpoints. We usedbeamsearchwithabeamsizeof4andlengthpenalty\u03b1 = 0.6[31]. Thesehyperparameters werechosenafterexperimentationonthedevelopmentset. Wesetthemaximumoutputlengthduring inferencetoinputlength+50,butterminateearlywhenpossible[31]. Table2summarizesourresultsandcomparesourtranslationqualityandtrainingcoststoothermodel architecturesfromtheliterature. Weestimatethenumberoffloatingpointoperationsusedtotraina modelbymultiplyingthetrainingtime,thenumberofGPUsused,andanestimateofthesustained single-precisionfloating-pointcapacityofeachGPU5. 6.2 ModelVariations ToevaluatetheimportanceofdifferentcomponentsoftheTransformer,wevariedourbasemodel indifferentways,measuringthechangeinperformanceonEnglish-to-Germantranslationonthe developmentset,newstest2013. Weusedbeamsearchasdescribedintheprevioussection,butno checkpointaveraging. WepresenttheseresultsinTable3. InTable3rows(A),wevarythenumberofattentionheadsandtheattentionkeyandvaluedimensions, keeping the amount of computation constant, as described in Section 3.2.2. While single-head attentionis0.9BLEUworsethanthebestsetting,qualityalsodropsoffwithtoomanyheads. 5Weusedvaluesof2.8,3.7,6.0and9.5TFLOPSforK80,K40,M40andP100,respectively. 8 Table3: VariationsontheTransformerarchitecture. Unlistedvaluesareidenticaltothoseofthebase model. AllmetricsareontheEnglish-to-Germantranslationdevelopmentset,newstest2013. Listed perplexitiesareper-wordpiece,accordingtoourbyte-pairencoding,andshouldnotbecomparedto per-wordperplexities. train PPL BLEU params N d d h d d P (cid:15) model ff k v drop ls steps (dev) (dev) \u00d7106 base 6 512 2048 8 64 64 0.1 0.1 100K 4.92 25.8 65 1 512 512 5.29 24.9 4 128 128 5.00 25.5 (A) 16 32 32 4.91 25.8 32 16 16 5.01 25.4 16 5.16 25.1 58 (B) 32 5.01 25.4 60 2 6.11 23.7 36 4 5.19 25.3 50 8 4.88 25.5 80 (C) 256 32 32 5.75 24.5 28 1024 128 128 4.66 26.0 168 1024 5.12 25.4 53 4096 4.75 26.2 90 0.0 5.77 24.6 0.2 4.95 25.5 (D) 0.0 4.67 25.3 0.2 5.47 25.7 (E) positionalembeddinginsteadofsinusoids 4.92 25.7 big 6 1024 4096 16 0.3 300K 4.33 26.4 213 InTable3rows(B),weobservethatreducingtheattentionkeysized hurtsmodelquality. This k suggests that determining compatibility is not easy and that a more sophisticated compatibility functionthandotproductmaybebeneficial. Wefurtherobserveinrows(C)and(D)that,asexpected, biggermodelsarebetter,anddropoutisveryhelpfulinavoidingover-fitting.Inrow(E)wereplaceour sinusoidalpositionalencodingwithlearnedpositionalembeddings[8],andobservenearlyidentical resultstothebasemodel.",
    "conclusion": "Inthiswork,wepresentedtheTransformer,thefirstsequencetransductionmodelbasedentirelyon attention,replacingtherecurrentlayersmostcommonlyusedinencoder-decoderarchitectureswith multi-headedself-attention. For translation tasks, the Transformer can be trained significantly faster than architectures based on recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014 English-to-Frenchtranslationtasks,weachieveanewstateoftheart. Intheformertaskourbest modeloutperformsevenallpreviouslyreportedensembles. Weareexcitedaboutthefutureofattention-basedmodelsandplantoapplythemtoothertasks. We plantoextendtheTransformertoproblemsinvolvinginputandoutputmodalitiesotherthantextand toinvestigatelocal,restrictedattentionmechanismstoefficientlyhandlelargeinputsandoutputs suchasimages,audioandvideo. Makinggenerationlesssequentialisanotherresearchgoalsofours. The code we used to train and evaluate our models is available at https://github.com/ tensorflow/tensor2tensor. Acknowledgements Wearegratefulto NalKalchbrennerand StephanGouwsfor theirfruitful comments,correctionsandinspiration. 9",
    "references": "[1] JimmyLeiBa,JamieRyanKiros,andGeoffreyEHinton. Layernormalization. arXivpreprint arXiv:1607.06450,2016. [2] DzmitryBahdanau,KyunghyunCho,andYoshuaBengio. Neuralmachinetranslationbyjointly learningtoalignandtranslate. CoRR,abs/1409.0473,2014. [3] DennyBritz,AnnaGoldie,Minh-ThangLuong,andQuocV.Le. Massiveexplorationofneural machinetranslationarchitectures. CoRR,abs/1703.03906,2017. [4] JianpengCheng,LiDong,andMirellaLapata. Longshort-termmemory-networksformachine reading. arXivpreprintarXiv:1601.06733,2016. [5] KyunghyunCho,BartvanMerrienboer,CaglarGulcehre,FethiBougares,HolgerSchwenk, andYoshuaBengio. Learningphraserepresentationsusingrnnencoder-decoderforstatistical machinetranslation. CoRR,abs/1406.1078,2014. [6] Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv preprintarXiv:1610.02357,2016. [7] JunyoungChung,\u00c7aglarG\u00fcl\u00e7ehre,KyunghyunCho,andYoshuaBengio. Empiricalevaluation ofgatedrecurrentneuralnetworksonsequencemodeling. CoRR,abs/1412.3555,2014. [8] JonasGehring,MichaelAuli,DavidGrangier,DenisYarats,andYannN.Dauphin. Convolu- tionalsequencetosequencelearning. arXivpreprintarXiv:1705.03122v2,2017. [9] Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint arXiv:1308.0850,2013. [10] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im- age recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,pages770\u2013778,2016. [11] SeppHochreiter,YoshuaBengio,PaoloFrasconi,andJ\u00fcrgenSchmidhuber. Gradientflowin recurrentnets: thedifficultyoflearninglong-termdependencies,2001. [12] Sepp Hochreiter and J\u00fcrgen Schmidhuber. Long short-term memory. Neural computation, 9(8):1735\u20131780,1997. [13] RafalJozefowicz,OriolVinyals,MikeSchuster,NoamShazeer,andYonghuiWu. Exploring thelimitsoflanguagemodeling. arXivpreprintarXiv:1602.02410,2016. [14] \u0141ukaszKaiserandIlyaSutskever. NeuralGPUslearnalgorithms. InInternationalConference onLearningRepresentations(ICLR),2016. [15] NalKalchbrenner,LasseEspeholt,KarenSimonyan,AaronvandenOord,AlexGraves,andKo- rayKavukcuoglu.Neuralmachinetranslationinlineartime.arXivpreprintarXiv:1610.10099v2, 2017. [16] YoonKim,CarlDenton,LuongHoang,andAlexanderM.Rush. Structuredattentionnetworks. InInternationalConferenceonLearningRepresentations,2017. [17] DiederikKingmaandJimmyBa. Adam: Amethodforstochasticoptimization. InICLR,2015. [18] OleksiiKuchaievandBorisGinsburg. FactorizationtricksforLSTMnetworks. arXivpreprint arXiv:1703.10722,2017. [19] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen Zhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint arXiv:1703.03130,2017. [20] SamyBengio\u0141ukaszKaiser. Canactivememoryreplaceattention? InAdvancesinNeural InformationProcessingSystems,(NIPS),2016. 10 [21] Minh-ThangLuong,HieuPham,andChristopherDManning. Effectiveapproachestoattention- basedneuralmachinetranslation. arXivpreprintarXiv:1508.04025,2015. [22] AnkurParikh,OscarT\u00e4ckstr\u00f6m,DipanjanDas,andJakobUszkoreit. Adecomposableattention model. InEmpiricalMethodsinNaturalLanguageProcessing,2016. [23] RomainPaulus,CaimingXiong,andRichardSocher. Adeepreinforcedmodelforabstractive summarization. arXivpreprintarXiv:1705.04304,2017. [24] OfirPressandLiorWolf. Usingtheoutputembeddingtoimprovelanguagemodels. arXiv preprintarXiv:1608.05859,2016. [25] RicoSennrich,BarryHaddow,andAlexandraBirch. Neuralmachinetranslationofrarewords withsubwordunits. arXivpreprintarXiv:1508.07909,2015. [26] NoamShazeer,AzaliaMirhoseini,KrzysztofMaziarz,AndyDavis,QuocLe,GeoffreyHinton, andJeffDean. Outrageouslylargeneuralnetworks: Thesparsely-gatedmixture-of-experts layer. arXivpreprintarXiv:1701.06538,2017. [27] NitishSrivastava,GeoffreyEHinton,AlexKrizhevsky,IlyaSutskever,andRuslanSalakhutdi- nov. Dropout: asimplewaytopreventneuralnetworksfromoverfitting. JournalofMachine LearningResearch,15(1):1929\u20131958,2014. [28] Sainbayar Sukhbaatar, arthur szlam, Jason Weston, and Rob Fergus. End-to-end memory networks. InC.Cortes, N.D.Lawrence, D.D.Lee, M.Sugiyama, andR.Garnett, editors, AdvancesinNeuralInformationProcessingSystems28,pages2440\u20132448.CurranAssociates, Inc.,2015. [29] IlyaSutskever,OriolVinyals,andQuocVVLe. Sequencetosequencelearningwithneural networks. InAdvancesinNeuralInformationProcessingSystems,pages3104\u20133112,2014. [30] ChristianSzegedy,VincentVanhoucke,SergeyIoffe,JonathonShlens,andZbigniewWojna. Rethinkingtheinceptionarchitectureforcomputervision. CoRR,abs/1512.00567,2015. [31] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang Macherey,MaximKrikun,YuanCao,QinGao,KlausMacherey,etal. Google\u2019sneuralmachine translationsystem: Bridgingthegapbetweenhumanandmachinetranslation. arXivpreprint arXiv:1609.08144,2016. [32] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with fast-forwardconnectionsforneuralmachinetranslation. CoRR,abs/1606.04199,2016. 11"
  },
  "pseudocode_blocks": [
    "theweightsonthe k values. Inpractice,wecomputetheattentionfunctiononasetofqueriessimultaneously,packedtogether intoamatrixQ. ThekeysandvaluesarealsopackedtogetherintomatricesK andV. Wecompute thematrixofoutputsas: QKT Attention(Q,K,V)=softmax( \u221a )V (1) d k Thetwomostcommonlyusedattentionfunctionsareadditiveattention[2],anddot-product(multi- plicative)attention. Dot-productattentionisidenticaltoouralgorithm,exceptforthescalingfactor of \u221a1 . Additiveattentioncomputesthecompatibilityfunctionusingafeed-forwardnetworkwith dk asinglehiddenlayer. Whilethetwoaresimilarintheoreticalcomplexity,dot-productattentionis muchfasterandmorespace-efficientinpractice,sinceitcanbeimplementedusinghighlyoptimized matrixmultiplicationcode. Whileforsmallvaluesofd thetwomechanismsperformsimilarly,additiveattentionoutperforms k dotproductattentionwithoutscalingforlargervaluesofd [3]. Wesuspectthatforlargevaluesof k d ,thedotproductsgrowlargeinmagnitude,pushingthesoftmaxfunctionintoregionswhereithas k extremelysmallgradients4. Tocounteractthiseffect,wescalethedotproductsby \u221a1 . dk 3.2.2 Multi-HeadAttention Insteadofperformingasingleattent",
    "hmidhuber. Gradientflowin recurrentnets: thedifficultyoflearninglong-termdependencies,2001. [12] Sepp Hochreiter and J\u00fcrgen Schmidhuber. Long short-term memory. Neural computation, 9(8):1735\u20131780,1997. [13] RafalJozefowicz,OriolVinyals,MikeSchuster,NoamShazeer,andYonghuiWu. Exploring thelimitsoflanguagemodeling. arXivpreprintarXiv:1602.02410,2016. [14] \u0141ukaszKaiserandIlyaSutskever. NeuralGPUslearnalgorithms. InInternationalConference onLearningRepresentations(ICLR),2016. [15] NalKalchbrenner,LasseEspeholt,KarenSimonyan,AaronvandenOord,AlexGraves,andKo- rayKavukcuoglu.Neuralmachinetranslationinlineartime.arXivpreprintarXiv:1610.10099v2, 2017. [16] YoonKim,CarlDenton,LuongHoang,andAlexanderM.Rush. Structuredattentionnetworks. InInternationalConferenceonLearningRepresentations,2017. [17] DiederikKingmaandJimmyBa. Adam: Amethodforstochasticoptimization. InICLR,2015. [18] OleksiiKuchaievandBorisGinsburg. FactorizationtricksforLSTMnetworks. arXivpreprint arXiv:1703.10722,2017. [19] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen Zhou, and Yoshua Bengio. A structured self-attentive senten"
  ],
  "raw_text_length": 29254
}